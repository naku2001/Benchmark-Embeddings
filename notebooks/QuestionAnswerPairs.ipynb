{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "25ff5ea0",
   "metadata": {},
   "outputs": [],
   "source": [
    "system_prompt = \"\"\"\n",
    "## Task description\n",
    "\n",
    "You are an expert at generating relevant questions given some chunk of text. Your task is to read that text and create one or more questions that accurately reflect the key points, or information contained within that chunk.\n",
    "\n",
    "This task is important for creating question-answer pairs that will be used to benchmark embedding models.\n",
    "\n",
    "## Output format\n",
    "\n",
    "You will output a JSON array of objects. Each object will have the following structure:\n",
    "\n",
    "{\n",
    "    \"question\": \"A question generated from the chunk of text.\",\n",
    "}\n",
    "\n",
    "## Examples\n",
    "\n",
    "### Example 1\n",
    "\n",
    "#### Text:\n",
    "\n",
    "bitsandbytes enables accessible large language models via k-bit quantization for PyTorch. bitsandbytes provides three main features for dramatically reducing memory consumption for inference and training:\n",
    "\n",
    "- 8-bit optimizers uses block-wise quantization to maintain 32-bit performance at a small fraction of the memory cost.\n",
    "- LLM.int8() or 8-bit quantization enables large language model inference with only half the required memory and without any performance degradation. This method is based on vector-wise quantization to quantize most features to 8-bits and separately treating outliers with 16-bit matrix multiplication.\n",
    "- QLoRA or 4-bit quantization enables large language model training with several memory-saving techniques that don’t compromise performance. This method quantizes a model to 4-bits and inserts a small set of trainable low-rank adaptation (LoRA) weights to allow training.\n",
    "\n",
    "#### Output:\n",
    "\n",
    "[\n",
    "    {\n",
    "        \"question\": \"What is the primary purpose of the bitsandbytes library?\"\n",
    "    },\n",
    "    {\n",
    "        \"question\": \"What are the three main features bitsandbytes provides for reducing memory consumption?\"\n",
    "    },\n",
    "    {\n",
    "        \"question\": \"How does the LLM.int8() feature work to enable 8-bit inference without performance degradation?\"\n",
    "    },\n",
    "    {\n",
    "        \"question\": \"What is QLoRA and how does it utilize 4-bit quantization for training?\"\n",
    "    },\n",
    "    {\n",
    "        \"question\": \"What quantization technique do the 8-bit optimizers in bitsandbytes use?\"\n",
    "    },\n",
    "    {\n",
    "        \"question\": \"Why is vector-wise quantization used in the LLM.int8() method, and how are outliers handled?\"\n",
    "    }\n",
    "]\n",
    "\n",
    "### Example 2\n",
    "\n",
    "#### Text:\n",
    "\n",
    "Creating your first star trail image\n",
    "How it works\n",
    "\n",
    "You’ve captured hundreds of photos, and now it’s time to blend them together to create your first star trail image. Each photo can be thought of as having two parts: the stars and the background.\n",
    "\n",
    "The background remains still, while the stars appear to move from frame to frame due to Earth’s rotation. Our goal is to keep the background consistent while revealing the stars’ motion across the sky.\n",
    "\n",
    "To do this, we gradually blend the images together to trace the path of each star. The pixels representing the background are mostly dark, while the ones representing the stars are bright.\n",
    "\n",
    "We use the lighten blending mode for this process, it takes the brighter value between a pixel in one image and the corresponding pixel in the next. This simple rule creates the illusion of continuous trails.\n",
    "\n",
    "If you can’t quite visualize it yet, don’t worry, I’ve included some illustrations to show exactly how this blending algorithm works.\n",
    "\n",
    "To demonstrate how the lighten blending mode works, I created five small 8×8 images. Each cell is colored either black (background) or white (stars).\n",
    "\n",
    "In the white cells, I’ve placed the number 1, which represents full brightness. In practice, each pixel in an 8-bit image stores a value between 0 and 255, where 255 corresponds to the maximum brightness. So, in this simplified example, 1 stands for 255.\n",
    "\n",
    "The black cells correspond to a brightness value of 0. For clarity, they are left empty in the diagram because they are greater in number than the white cells.\n",
    "\n",
    "In the first image, there are three stars. From one frame to the next, each star moves one pixel to the right and one pixel down. By the final frame, only one star remains visible, as the other two have moved outside the 8×8 grid.\n",
    "\n",
    "Now, let’s apply the lighten blending mode to the first two images. In the illustration, you’ll see them represented as inputs to the max() function. This function compares the pixel values from both images and keeps the brighter one for each position. The resulting image is a blend of the two, showing all the stars that were visible in either frame.\n",
    "\n",
    "The blended image becomes the new input for the next iteration of the max() function. The third image is then used as the second argument. Blend these two together, and repeat the process with the remaining images until you reach the fifth one.\n",
    "\n",
    "The final blended result is your complete star trail image, showing the continuous paths traced by the stars over time.\n",
    "\n",
    "#### Output:\n",
    "\n",
    "[\n",
    "    {\n",
    "        \"question\": \"What is the primary goal when blending photos to create a star trail image?\"\n",
    "    },\n",
    "    {\n",
    "        \"question\": \"Why do the stars appear to move from one photo to the next?\"\n",
    "    },\n",
    "        {\n",
    "        \"question\": \"What are the two main components of each image used to create a star trail photo?\"\n",
    "    },\n",
    "    {\n",
    "        \"question\": \"What specific blending mode is used to create the star trail effect?\"\n",
    "    },\n",
    "    {\n",
    "        \"question\": \"In the simplified 8×8 grid example, what do the white cells and black cells represent in terms of pixel brightness?\"\n",
    "    },\n",
    "    {\n",
    "        \"question\": \"What mathematical function is used in the illustration to represent the lighten blending mode?\"\n",
    "    },\n",
    "    {\n",
    "        \"question\": \"How does on a pixel level?\"\n",
    "    },\n",
    "    {\n",
    "        \"question\": \"Describe the iterative process of blending the images to create the final star trail.\"\n",
    "    },\n",
    "]\n",
    "\n",
    "## Notes\n",
    "\n",
    "- Don't generate questions that are similar to each other.\n",
    "- Questions should be relevant to the content of the chunk of text.\n",
    "- Generate only a JSON array as specified in the output format.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "21ff0524",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic import BaseModel\n",
    "\n",
    "\n",
    "class Question(BaseModel):\n",
    "    question: str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac7bda30",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Missing key inputs argument! To use the Google AI API, provide (`api_key`) arguments. To use the Google Cloud API, provide (`vertexai`, `project` & `location`) arguments.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipython-input-2744964988.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mgoogle\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mgenai\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mgoogle_client\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgenai\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mClient\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mgenerate_questions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext_chunk\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0msystem_prompt\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmodel_name\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mstr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"gemini-2.5-flash\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m->\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mQuestion\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/google/genai/client.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, vertexai, api_key, credentials, project, location, debug_config, http_options)\u001b[0m\n\u001b[1;32m    424\u001b[0m         \u001b[0mhttp_options\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mHttpOptions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbase_url\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbase_url\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    425\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 426\u001b[0;31m     self._api_client = self._get_api_client(\n\u001b[0m\u001b[1;32m    427\u001b[0m         \u001b[0mvertexai\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvertexai\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    428\u001b[0m         \u001b[0mapi_key\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mapi_key\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/google/genai/client.py\u001b[0m in \u001b[0;36m_get_api_client\u001b[0;34m(vertexai, api_key, credentials, project, location, debug_config, http_options)\u001b[0m\n\u001b[1;32m    472\u001b[0m       )\n\u001b[1;32m    473\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 474\u001b[0;31m     return BaseApiClient(\n\u001b[0m\u001b[1;32m    475\u001b[0m         \u001b[0mvertexai\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvertexai\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    476\u001b[0m         \u001b[0mapi_key\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mapi_key\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/google/genai/_api_client.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, vertexai, api_key, credentials, project, location, http_options)\u001b[0m\n\u001b[1;32m    688\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# Implicit initialization or missing arguments.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    689\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapi_key\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 690\u001b[0;31m         raise ValueError(\n\u001b[0m\u001b[1;32m    691\u001b[0m             \u001b[0;34m'Missing key inputs argument! To use the Google AI API,'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    692\u001b[0m             \u001b[0;34m' provide (`api_key`) arguments. To use the Google Cloud API,'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Missing key inputs argument! To use the Google AI API, provide (`api_key`) arguments. To use the Google Cloud API, provide (`vertexai`, `project` & `location`) arguments."
     ]
    }
   ],
   "source": [
    "from google import genai\n",
    "google_client = genai.Client(api_key=\"AIzaSyC01YDM8z8VZq7hv4wZLoRhTR8Zu_WYDB4\")\n",
    "\n",
    "def generate_questions(text_chunk:str,system_prompt:str,model_name:str = \"gemini-2.5-flash\")-> list[Question]:\n",
    "    try:\n",
    "        response = google_client.models.generate_content(model = model_name,contents=[system_prompt,text_chunk,],config={\"response_mime_type\": \"application/json\",\n",
    "                \"response_schema\": list[Question],\n",
    "                \"max_output_tokens\": 65_536,},)\n",
    "        questions:list[Question] = response.parsed\n",
    "        return questions\n",
    "    except Exception as e:\n",
    "        print(f\"Error generating questions:{e}\")\n",
    "        return []\n",
    "                       \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
